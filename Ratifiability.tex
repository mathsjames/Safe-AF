\documentclass{article}

\usepackage[a4paper]{geometry}

\pdfoutput=1
\usepackage{amssymb}	% Allows use of AMS's mathematical symbols
\usepackage{latexsym}	% Allows use of old latex symbols
\usepackage{amsmath}
\usepackage{amsthm}


\usepackage{subfig}


%define absolute value
\usepackage{mathtools}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}

\usepackage{multirow}

\usepackage{microtype}  % Kann einige Badnesses eliminieren

\usepackage{hyperref}   % Auskommentieren, um Referenzen etc. anklickbar zu machen.

\usepackage{cleveref}

\usepackage{array}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{N}{@{}m{0pt}@{}}
%FROM: http://tex.stackexchange.com/questions/159257/increase-latex-table-row-height

\usepackage[backend=biber,natbib=true,style=authoryear-comp,citestyle=authoryear]{biblatex}
%natbib=true,style=authortitle-icomp,style=authoryear
%\usepackage{cite}
%\usepackage[authoryear]{natbib}
%authortitle-icomp

%% For old versions of latex, replace the above three lines by
%\documentstyle[amssymbols]{article}

\usepackage[utf8]{inputenc} %Auch utf8 kann eine gute Idee sein
\usepackage{csquotes}
\usepackage[english]{babel}

\usepackage{graphicx}

\usepackage{colonequals}
\newcommand*{\logeq}{\ratio\Leftrightarrow}

\usepackage{mathtools}

\DeclareMathOperator*{\cov}{cov}
\DeclareMathOperator*{\argmax}{arg\,max}


\usepackage{xcolor}

\usepackage{pgfplots}
\pgfplotsset{compat=1.13}


\usepackage{tikz}
\usetikzlibrary{arrows,automata}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows}
\usetikzlibrary{trees}
\tikzset{main node/.style={circle,fill=blue!20,draw,minimum size=1cm,inner sep=0pt},
            }


\newcommand{\TRUE}{\textsf{T}}
\newcommand{\FALSE}{\textsf{F}}

%From http://tex.stackexchange.com/questions/588/how-can-i-change-the-margins-for-only-part-of-the-text
\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist


\usepackage{multirow,array}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newtheorem{thm}{Theorem} %Theoreme
\newtheorem{corollary}[thm]{Corollary}

\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}

\title{Relation to ratifiability}

\begin{document}
\maketitle

For now, I'll only consider games without anthropics and without different situations. The agent submits a probability distribution once, an action is sampled from it and then the environment behaves in some way depending on that action and probability distribution.

A policy $\pi:\mathbb{N}\times \mathbb{R}^A \rightsquigarrow A=\{a_1,...,a_n\}$ is a function mapping a time step and a mapping of empirical expected values / Q-values onto probability distributions over actions. Let $P_i$ denote the sequence of probability distributions over $A$. Note that the $P_i$ are random variables. Let $Q_i\in \mathbb{R}^A$ be the sequence of empirical EVs, again random variables. Let $U:A\times \left[0,1 \right]^A \rightarrow \mathbb{R}$ be the actual EV given an action $a$ and probability distribution $p$, where $U$ is continuous in $p$. (Defining $U$ such that it also assigns expected utilities to actions that are assigned zero probability is important for discussing the behavior of $U$ in the limit.) (Unfortunately, this function cannot be as easily defined for problems involving anthropics and so forth.)

We say a sequence of random variables $(X_i)_{i\in\mathbb{N}}$ converges almost surely to $x$, or $X_i\underset{\text{a.s.}}{\rightarrow} x$, if ... (see \url{https://en.wikipedia.org/wiki/Convergence_of_random_variables#Almost_sure_convergence}). Conversely, we say that the sequence converges to $x$ with positive probability, or $X_i\underset{\text{w.p.p.}}{\rightarrow} x$, if $P(\lim_{i\rightarrow \infty } X_n = x)>0$.

\begin{thm}
Let $\pi$ be a policy s.t. $\pi(i,\cdot)$ is continuous for all $i\in \mathbb{N}$ . For each $q\in \mathbb{R}^A$ and each $j\in \{1,...,n\}$ let
\begin{equation}
\pi (i, q) \rightarrow \pi^\infty (q(a_1),...,q(a_n)).
\end{equation}
Assume that $\pi^\infty$ \enquote{other things equal always gives higher probabilities to the actions with higher utility}.
Furthermore, let $U(a,p)$ be continuous in $p$ for each $a\in A$ and let $P_i \underset{w.p.p.}{\rightarrow} \mathbf{p}$. Then there are $b_1,...,b_n$ such that $b_j=a_j$ if $a_j\in supp (\mathbf{p})=\{x\in A\mid \mathbf{p}(x)>0 \}$ and $b_j<\min_{a_k\in supp (\mathbf{p}) } U(a_k,\mathbf{p})$ such that for all $a_j\in supp (\mathbf{p})$, it is
\begin{equation}
\mathbf{p}(a_j)=\pi^\infty (b_1,...,b_n) (a_j). %TODO: unclear what happens with this term if $\mathbf{p}(a_k)=0$.
\end{equation}
\end{thm}

\begin{proof}
\textbf{1.} We first show that for all $a_j\in supp (\mathbf{p})$, if indeed $P_i\rightarrow \mathbf{p}$, then 
\begin{equation}\label{eq:proof-part-one-main-eq}
Q_i(a_j)\underset{\text{a.s.}}{\rightarrow} U(a_j,\mathbf{p}).
\end{equation}
Hence we define for $a_j\in supp (\mathbf{p})$: $b_j=U(a_j,\mathbf{p})$

\textbf{2.} If $P_i\rightarrow \mathbf{p}$, then because $\pi$ is continuous and prefers better options, there must be an $N$ such that for all $i>N$ and all $a\notin supp (\mathbf{p})$ it must be
\begin{equation}\label{eq:proof-part-one-main-eq}
Q_i(a)< \min_{a_k\in supp (\mathbf{p}) } U(a_k,\mathbf{p}).
\end{equation}

\textbf{3.} As $P_i \rightarrow \mathbf{p}$, $Q_i(a_j)$ almost surely converges to some value even for $a_j\notin supp(\mathbf{p})$. Because of step 2, these values are smaller than $\min_{a_k\in supp (\mathbf{p}) } U(a_k,\mathbf{p})$. Hence, we will use these limits as $b_j$.

\textbf{4.} If $Q_i(a_j)\rightarrow b_j$ for all $a_j\in A$, then
\begin{equation}\label{eq:proof-part-one-main-eq}
P_i \rightarrow \pi^\infty (b_1,...,b_n).
\end{equation}

\textbf{5.} From the conditions of the hypothesis and steps 1--4, it follows that with positive probability, it is both $P_i\rightarrow \mathbf{p}$ and for all $a_j\in supp(\mathbf{p})$
\begin{equation}\label{eq:proof-part-one-main-eq}
P_i(a_j) \rightarrow \pi^\infty  (b_1,...,b_n)(a_j).
\end{equation}
Hence it must be for all $a_j\in supp(\mathbf{p})$
\begin{equation}
\mathbf{p}(a_j)=\pi^\infty (b_1,...,b_n)(a_j),
\end{equation}
where the $b_j$ satisfy the claims made in the hypothesis.
\end{proof}

\begin{corollary}[Ratifiability]
Same conditions as for previous theorem, but also assume that $\pi^\infty$ doesn't explore, i.e. that
\begin{equation}
\pi^\infty(v_1,...,v_n)(a_j) >0 \iff j\in \argmax_k v_k.
\end{equation}
Then $U(a,\mathbf{p})$ is constant over $a\in supp(\mathbf{p})$.
\end{corollary}

Notes for generalization:
\begin{itemize}
\item Probably, I could easily generalize this to expected values conditional on some observation.
\item If you explore all options infinitely often almost surely, you almost surely converge to a \enquote{really ratifiable} solution. If you don't explore all options infinitely often almost surely, there is a positive probability that it doesn't converge to a \enquote{really ratifiable} option.
\item If it doesn't converge, then there is still ratifiability of some frequency construct, perhaps?
\item Include anthropic cases.
\end{itemize}

\section*{Ratifiability of frequencies}

Even if the probabilities do not converge at all, the frequencies of actions over many turns usually do. In fact, they often converge to ratifiable ones. E.g., in Death in Damascus, even if the probabilities do not converge, the frequencies converge to the ratifiable 50-50. But this does not seem to be true in general, even if the other prerequisites of the theorem are met. Roughly, the reason is the following: the frequencies arising from applying the learning algorithm are based on the success of actions for the success probabilities, rather than that frequency itself. So, the frequencies can converge to 50-50 based on how the actions behave if the probability is far removed from 50-50, even if at a (hypothetical) probability of 50-50, one of the actions is better than the other.

\section*{Some references on ratifiability}

Some versions of the tickle defense are based on ratifiability, including 


\end{document}
