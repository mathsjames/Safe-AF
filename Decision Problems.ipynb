{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Absent_Minded_Driver:\n",
    "    \n",
    "    # Full specification of the absent-minded driver problem\n",
    "    \n",
    "    def __init__(self):   \n",
    "        self.actions = [\"CONT\", \"EXIT\"]\n",
    "        self.states = [\"Intersection_1\", \"Intersection_2\", \"Stop_A\", \"Stop_B\", \"Stop_C\"]\n",
    "        self.end_states = [\"Stop_A\", \"Stop_B\", \"Stop_C\"]\n",
    "        self.epistemic_states = [\"Intersection\", \"Stop_A\", \"Stop_B\", \"Stop_C\"]\n",
    "        \n",
    "        self.state_to_epistemic_state_dict = {\"Intersection_1\":\"Intersection\",\n",
    "                                \"Intersection_2\":\"Intersection\", \n",
    "                                \"Stop_A\":\"Stop_A\", \n",
    "                                \"Stop_B\":\"Stop_B\", \n",
    "                                \"Stop_C\":\"Stop_C\"\n",
    "                                }\n",
    "        \n",
    "        #self.epistemic_state_to_state_dict = {\"Intersection\":[\"Intersection_1\", \"Intersection_2\"],\n",
    "        #                                    \"Stop_A\":[\"Stop_A\"], \n",
    "        #                                    \"Stop_B\":[\"Stop_B\"], \n",
    "        #                                    \"Stop_C\":[\"Stop_C\"]\n",
    "        #                                    }\n",
    "    \n",
    "        self.causation_dict = {(\"Intersection_1\", \"EXIT\"):\"Stop_A\", \n",
    "                              (\"Intersection_1\", \"CONT\"):\"Intersection_2\", \n",
    "                              (\"Intersection_2\", \"EXIT\"):\"Stop_B\", \n",
    "                              (\"Intersection_2\", \"CONT\"):\"Stop_C\", \n",
    "                              }\n",
    "\n",
    "        self.utility_dict = {\"Stop_A\":0,\n",
    "                        \"Stop_B\":4,\n",
    "                        \"Stop_C\":1,\n",
    "                        }\n",
    "        \n",
    "        self.start_state = \"Intersection_1\"\n",
    "        #self.reset()\n",
    "\n",
    "    def reset(self, agent):\n",
    "        self.finished = False\n",
    "        self.state = self.start_state\n",
    "        \n",
    "    def do(self, action, distribution=[1]):\n",
    "        \n",
    "        if action in self.actions:\n",
    "            self.state = self.cause(self.state, action)\n",
    "            utility = self.utility(self.state)\n",
    "        else:\n",
    "            assert False\n",
    "            \n",
    "        if self.state in self.end_states:\n",
    "            self.finished = True\n",
    "        \n",
    "        return (self.epistemic_state(), utility)\n",
    "        \n",
    "    def utility(self, state):\n",
    "        try:\n",
    "            return self.utility_dict[state]\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def epistemic_state(self):\n",
    "        return self.state_to_epistemic_state_dict[self.state]\n",
    "        \n",
    "    def cause(self, state, action):\n",
    "        try:\n",
    "            return self.causation_dict[(state, action)]\n",
    "        except:\n",
    "            return state\n",
    "            \n",
    "    #def possible_states(self, epistemic_state):\n",
    "    #    return self.inverse_epistemic_state_dict[epistemic_state]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evidential_Blackmail:\n",
    "    \n",
    "    # Full specification of the absent-minded driver problem\n",
    "    \n",
    "    def __init__(self):   \n",
    "        self.actions = [\"PAY\", \"DONT\"]\n",
    "        \n",
    "        self.states = [\"crash+blackmail\", \"no crash+blackmail\", \n",
    "                       \"crash+no blackmail\", \"no crash+no blackmail\",\n",
    "                       \"crash+pay\", \"crash+no pay\", \"no crash+pay\", \"no crash+no pay\"]\n",
    "        \n",
    "        self.end_states = [\"crash+pay\", \"crash+no pay\", \"no crash+pay\", \"no crash+no pay\"]\n",
    "        \n",
    "        self.epistemic_states = [\"Blackmail\", \"No Blackmail\", \"END\"]\n",
    "        \n",
    "        self.state_to_epistemic_state_dict = {\"crash+blackmail\":\"Blackmail\",\n",
    "                                              \"no crash+blackmail\":\"Blackmail\",\n",
    "                                              \"crash+no blackmail\":\"No Blackmail\",\n",
    "                                              \"no crash+no blackmail\":\"No Blackmail\",\n",
    "                                              \"crash+pay\":\"END\",\n",
    "                                              \"crash+no pay\":\"END\",\n",
    "                                              \"no crash+pay\":\"END\",\n",
    "                                              \"no crash+no pay\":\"END\",\n",
    "                                             }\n",
    "    \n",
    "        self.causation_dict = {(\"crash+blackmail\", \"PAY\"):\"crash+pay\", \n",
    "                               (\"crash+blackmail\", \"DONT\"):\"crash+no pay\", \n",
    "                               (\"no crash+blackmail\", \"PAY\"):\"no crash+pay\", \n",
    "                               (\"no crash+blackmail\", \"DONT\"):\"no crash+no pay\", \n",
    "                               (\"crash+no blackmail\", \"PAY\"):\"crash+pay\",\n",
    "                               (\"crash+no blackmail\", \"DONT\"):\"crash+no pay\", \n",
    "                               (\"no crash+no blackmail\", \"PAY\"):\"no crash+pay\", \n",
    "                               (\"no crash+no blackmail\", \"DONT\"):\"no crash+no pay\", \n",
    "                              }\n",
    "\n",
    "        self.utility_dict = {\"no crash+no pay\":11,\n",
    "                             \"no crash+pay\":10,\n",
    "                             \"crash+no pay\":1,\n",
    "                             \"crash+pay\":0\n",
    "                            }\n",
    "        \n",
    "        #self.reset()\n",
    "\n",
    "    def reset(self, agent):\n",
    "        self.finished = False\n",
    "        crash = np.random.choice([True, False], 1, p=[0.5, 0.5])[0]\n",
    "        \n",
    "        if crash:\n",
    "            self.state = \"crash+no blackmail\"\n",
    "            \n",
    "        else:\n",
    "            if agent.decide(\"Blackmail\") == \"PAY\":\n",
    "                self.state = \"no crash+blackmail\"\n",
    "            else:\n",
    "                self.state = \"no crash+no blackmail\"\n",
    "\n",
    "        \n",
    "    def do(self, action, distribution=[1]):\n",
    "        \n",
    "        if action in self.actions:\n",
    "            self.state = self.cause(self.state, action)\n",
    "            utility = self.utility(self.state)\n",
    "        else:\n",
    "            assert False\n",
    "            \n",
    "        if self.state in self.end_states:\n",
    "            self.finished = True\n",
    "        \n",
    "        return (self.epistemic_state(), utility)\n",
    "        \n",
    "    def utility(self, state):\n",
    "        try:\n",
    "            return self.utility_dict[state]\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def epistemic_state(self):\n",
    "        return self.state_to_epistemic_state_dict[self.state]\n",
    "        \n",
    "    def cause(self, state, action):\n",
    "        try:\n",
    "            return self.causation_dict[(state, action)]\n",
    "        except:\n",
    "            return state\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_probability(exp_util):\n",
    "    return softmax(exp_util)\n",
    "\n",
    "def normalise(x):\n",
    "    ans = x.copy()\n",
    "    for i in range(len(x)):\n",
    "        ans[i] /= sum(x) \n",
    "    return ans\n",
    "\n",
    "def softmax(x):\n",
    "    x = [i/10.0 for i in x]\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Action_Agent:\n",
    "    \n",
    "    # Agent that iterates over actions and selects one that has given it a lot of utility in the past\n",
    "\n",
    "    def __init__(self, decision_problem):  \n",
    "\n",
    "        self.DP = decision_problem\n",
    "        self.actions = self.DP.actions\n",
    "        \n",
    "        temp = {action:1 for action in self.actions}\n",
    "        self.expected_utility = {es:temp.copy() for es in self.DP.epistemic_states}\n",
    "        self.times_action_taken = {es:temp.copy() for es in self.DP.epistemic_states}\n",
    "        \n",
    "    def get_action_probabilities(self, epistemic_state):\n",
    "        \n",
    "        xp = [self.expected_utility[epistemic_state][action] for action in self.actions]\n",
    "        action_probabilities = to_probability(xp)\n",
    "        return action_probabilities\n",
    "            \n",
    "    def decide(self, epistemic_state):\n",
    "        \n",
    "        action_probabilities = self.get_action_probabilities(epistemic_state)\n",
    "        return np.random.choice(self.actions, 1, p=action_probabilities)[0]\n",
    "    \n",
    "    def learn_from(self, epistemic_state, action, utility):\n",
    "        \n",
    "        i = self.times_action_taken[epistemic_state][action]\n",
    "        exp = self.expected_utility[epistemic_state][action]\n",
    "        self.expected_utility[epistemic_state][action] = (utility+exp*i)/(i+1.0)\n",
    "        self.times_action_taken[epistemic_state][action] += 1\n",
    "                \n",
    "    def play(self, iterations):\n",
    "        \n",
    "        utility_record = []\n",
    "        for i in range(iterations):          \n",
    "\n",
    "            self.DP.reset(self)\n",
    "            total_utility = 0\n",
    "            history = []\n",
    "            \n",
    "            while not self.DP.finished:\n",
    "                \n",
    "                epistemic_state = self.DP.epistemic_state()\n",
    "                action = self.decide(epistemic_state)\n",
    "                action_probabilities = self.get_action_probabilities(epistemic_state)\n",
    "                _, new_utility = self.DP.do(action, action_probabilities)\n",
    "                \n",
    "                history.append((epistemic_state, action, new_utility))\n",
    "                total_utility += new_utility\n",
    "\n",
    "            average_action_utility = total_utility/len(history)\n",
    "            \n",
    "            for epistemic_state, action, _ in history:\n",
    "                self.learn_from(epistemic_state, action, average_action_utility)\n",
    "                \n",
    "            utility_record.append(total_utility)\n",
    "            \n",
    "        return utility_record\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Naive_Action_Agent:\n",
    "    \n",
    "    # Agent that iterates over actions, but only rewards the action immideately preceding the reward\n",
    "\n",
    "    def __init__(self, decision_problem):  \n",
    "\n",
    "        self.DP = decision_problem\n",
    "        self.actions = self.DP.actions\n",
    "        \n",
    "        temp = {action:1 for action in self.actions}\n",
    "        self.expected_utility = {es:temp.copy() for es in self.DP.epistemic_states}\n",
    "        self.times_action_taken = {es:temp.copy() for es in self.DP.epistemic_states}\n",
    "        \n",
    "    def get_action_probabilities(self, epistemic_state):\n",
    "        \n",
    "        xp = [self.expected_utility[epistemic_state][action] for action in self.actions]\n",
    "        action_probabilities = to_probability(xp)\n",
    "        return action_probabilities\n",
    "            \n",
    "    def decide(self, epistemic_state):\n",
    "        \n",
    "        action_probabilities = self.get_action_probabilities(epistemic_state)      \n",
    "        return np.random.choice(self.actions, 1, p=action_probabilities)[0]\n",
    "    \n",
    "    def learn_from(self, epistemic_state, action, utility):\n",
    "        \n",
    "        i = self.times_action_taken[epistemic_state][action]\n",
    "        exp = self.expected_utility[epistemic_state][action]\n",
    "        self.expected_utility[epistemic_state][action] = (utility+exp*i)/(i+1)\n",
    "        self.times_action_taken[epistemic_state][action] += 1\n",
    "        \n",
    "    def play(self, iterations):\n",
    "        \n",
    "        utility_record = []\n",
    "        for i in range(iterations):          \n",
    "\n",
    "            self.DP.reset(self)\n",
    "            total_utility = 0\n",
    "            history = []\n",
    "            \n",
    "            while not self.DP.finished:\n",
    "\n",
    "                epistemic_state = self.DP.epistemic_state()\n",
    "                action = self.decide(epistemic_state)\n",
    "                action_probabilities = self.get_action_probabilities(epistemic_state)\n",
    "                _, new_utility = self.DP.do(action, action_probabilities)\n",
    "                \n",
    "                self.learn_from(epistemic_state, action, new_utility)\n",
    "                \n",
    "                total_utility += new_utility\n",
    "                \n",
    "            utility_record.append(total_utility)\n",
    "            \n",
    "        return utility_record\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Agent:\n",
    "    \n",
    "    # Agent that iterated over policies\n",
    "\n",
    "    def __init__(self, decision_problem):  \n",
    "\n",
    "        self.DP = decision_problem\n",
    "        self.actions = self.DP.actions\n",
    "        \n",
    "        self.policies = [str(x) for x in np.arange(0.0, 1.0, 0.05)] # assumes only two possible actions atm\n",
    "        \n",
    "        temp = {policy:1 for policy in self.policies}\n",
    "        self.expected_utility = {es:temp.copy() for es in self.DP.epistemic_states}\n",
    "        self.times_action_taken = {es:temp.copy() for es in self.DP.epistemic_states}\n",
    "        \n",
    "    def get_action_probabilities(self, epistemic_state):\n",
    "        \n",
    "        xp = [self.expected_utility[epistemic_state][policy] for policy in self.policies]\n",
    "        policy_probabilities = to_probability(xp)\n",
    "        p = sum(np.arange(0.0, 1.0, 0.05)*policy_probabilities)\n",
    "        \n",
    "        return [p, 1-p]\n",
    "            \n",
    "    def decide(self, epistemic_state):\n",
    "        # not used for this agent\n",
    "        \n",
    "        policy = self.get_action_probabilities(epistemic_state)\n",
    "        return np.random.choice(self.actions, 1, p=policy)[0]\n",
    "    \n",
    "    def learn_from(self, epistemic_state, policy, utility):\n",
    "        \n",
    "        i = self.times_action_taken[epistemic_state][policy]\n",
    "        exp = self.expected_utility[epistemic_state][policy]\n",
    "        self.expected_utility[epistemic_state][policy] = (utility+exp*i)/(i+1)\n",
    "        self.times_action_taken[epistemic_state][policy] += 1\n",
    "    \n",
    "    def choose_policy(self, epistemic_state):\n",
    "        \n",
    "        xp = [self.expected_utility[epistemic_state][policy] for policy in self.policies]\n",
    "        policy_probabilities = to_probability(xp)\n",
    "        #policy_probabilities = test_prob_arr\n",
    "        policy = np.random.choice(self.policies, 1, p=policy_probabilities)[0]\n",
    "        \n",
    "        return policy\n",
    "                \n",
    "    def play(self, iterations):\n",
    "    \n",
    "        utility_record = []\n",
    "        for i in range(iterations):          \n",
    "\n",
    "            self.DP.reset(self)\n",
    "            total_utility = 0\n",
    "            #history = []\n",
    "            \n",
    "            policy_dict = {es:self.choose_policy(es) for es in self.DP.epistemic_states}\n",
    "            \n",
    "            while not self.DP.finished:\n",
    "                \n",
    "                epistemic_state = self.DP.epistemic_state()\n",
    "                policy = policy_dict[epistemic_state]\n",
    "                action = np.random.choice(self.actions, 1, p=[float(policy), 1-float(policy)])[0]\n",
    "                _, new_utility = self.DP.do(action, policy)\n",
    "                \n",
    "                #history.append((epistemic_state, policy, new_utility))\n",
    "                \n",
    "                total_utility += new_utility\n",
    "                \n",
    "            for es in self.DP.epistemic_states:\n",
    "                pc = policy_dict[es]\n",
    "                self.learn_from(es, pc, total_utility)\n",
    "\n",
    "            #average_policy_utility = total_utility/len(history)\n",
    "            #for epistemic_state, policy, _ in history:\n",
    "            #    self.learn_from(epistemic_state, policy, average_policy_utility)\n",
    "                \n",
    "            utility_record.append(total_utility)\n",
    "            \n",
    "        return utility_record\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2618\n",
      "0.507997316232\n",
      "\n",
      "1.2735\n",
      "0.50791119912\n",
      "\n",
      "1.2487\n",
      "0.508196909592\n",
      "\n",
      "1.2712\n",
      "0.507851333041\n",
      "\n",
      "1.2572\n",
      "0.508404768772\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    AMD = Absent_Minded_Driver()\n",
    "    agent = Action_Agent(AMD)\n",
    "\n",
    "    print(np.mean(agent.play(10000)))\n",
    "    # optimal value: 4/3\n",
    "\n",
    "    print(agent.get_action_probabilities(\"Intersection\")[0])\n",
    "    # optimal value: 2/3\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2227\n",
      "0.4758811679\n",
      "\n",
      "1.2642\n",
      "0.474404392337\n",
      "\n",
      "1.2359\n",
      "0.475402064757\n",
      "\n",
      "1.2214\n",
      "0.475946006534\n",
      "\n",
      "1.1979\n",
      "0.476765568304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    AMD = Absent_Minded_Driver()\n",
    "    agent = Naive_Action_Agent(AMD)\n",
    "\n",
    "    print(np.mean(agent.play(10000)))\n",
    "    # optimal value: 4/3\n",
    "    \n",
    "    print(agent.get_action_probabilities(\"Intersection\")[0])\n",
    "    # optimal value: 2/3\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9831\n",
      "0.484216925736\n",
      "\n",
      "0.9986\n",
      "0.484508410309\n",
      "\n",
      "1.0065\n",
      "0.484482036277\n",
      "\n",
      "1.0107\n",
      "0.484495306922\n",
      "\n",
      "0.9617\n",
      "0.484501916129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AMD = Absent_Minded_Driver()\n",
    "agent = Policy_Agent(AMD)\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    print(np.mean(agent.play(10000)))\n",
    "    # optimal value: 4/3\n",
    "    \n",
    "    print(agent.get_action_probabilities(\"Intersection\")[0])\n",
    "    # optimal value: 2/3\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.5568\n",
      "0.475014891802\n",
      "0.476538777514\n",
      "\n",
      "5.5941\n",
      "0.475018309396\n",
      "0.471766838764\n",
      "\n",
      "5.4328\n",
      "0.475014897395\n",
      "0.473873518892\n",
      "\n",
      "5.5592\n",
      "0.475017622465\n",
      "0.476112635645\n",
      "\n",
      "5.5597\n",
      "0.475012112494\n",
      "0.474844463627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    EB = Evidential_Blackmail()\n",
    "    agent = Action_Agent(EB)\n",
    "\n",
    "    print(np.mean(agent.play(10000)))\n",
    "\n",
    "    print(agent.get_action_probabilities(\"Blackmail\")[0])\n",
    "    print(agent.get_action_probabilities(\"No Blackmail\")[0])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.5456\n",
      "0.475027877184\n",
      "0.478972403014\n",
      "\n",
      "5.5418\n",
      "0.475024693499\n",
      "0.470364282871\n",
      "\n",
      "5.3883\n",
      "0.47503403463\n",
      "0.476956420957\n",
      "\n",
      "5.5375\n",
      "0.475022993943\n",
      "0.472022257449\n",
      "\n",
      "5.5869\n",
      "0.475003438878\n",
      "0.476092235975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    EB = Evidential_Blackmail()\n",
    "    agent = Naive_Action_Agent(EB)\n",
    "\n",
    "    print(np.mean(agent.play(10000)))\n",
    "\n",
    "    print(agent.get_action_probabilities(\"Blackmail\")[0])\n",
    "    print(agent.get_action_probabilities(\"No Blackmail\")[0])\n",
    "    \n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.4781\n",
      "0.474453519622\n",
      "0.468667271708\n",
      "\n",
      "5.5445\n",
      "0.472710655986\n",
      "0.468298815595\n",
      "\n",
      "5.6398\n",
      "0.473266916141\n",
      "0.466815931958\n",
      "\n",
      "5.5072\n",
      "0.473972252931\n",
      "0.467717416996\n",
      "\n",
      "5.5111\n",
      "0.475044743085\n",
      "0.470855925838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    EB = Evidential_Blackmail()\n",
    "    agent = Policy_Agent(EB)\n",
    "\n",
    "    print(np.mean(agent.play(10000)))\n",
    "\n",
    "    print(agent.get_action_probabilities(\"Blackmail\")[0])\n",
    "    print(agent.get_action_probabilities(\"No Blackmail\")[0])\n",
    "    \n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
